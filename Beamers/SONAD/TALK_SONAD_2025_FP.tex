\documentclass{beamer}

\usetheme{McMaster}
\usepackage{math}

%===Specific for this talk===%
\usepackage{algorithm,algorithmicx,algpseudocode}
\newtheorem{proposition}{Proposition}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\adj}{adj}
\usepackage{tikz, pgfplots}
\usetikzlibrary{decorations.pathreplacing,positioning,calc,intersections,3d,shapes.geometric,shapes,chains,math,fit,backgrounds}

\title{Accelerating fixed point iterations with Newton's method}
\author{Conor McCoid}
\institute{McMaster University}
\date{May 30th, 2025}

\begin{document}

\maketitle

\section{Fixed point iterations}

\begin{frame}
\frametitle{Fixed points in 1D}

\begin{columns}
\begin{column}{0.5\textwidth}
The fixed point of a function $g(x)$ is a point $x^*$ such that
\begin{equation*}
	g(x^*) = x^*.
\end{equation*}

These are also described as the intersection between the lines $y=g(x)$ and $y=x$.
\end{column}
\begin{column}{0.5\textwidth}
	\begin{figure}
		\centering
		\begin{tikzpicture}[domain=-2:2]
			\draw[blue] plot (\x,\x) node[right] {$y=x$};
			\draw[red, domain=-2:2] plot (\x,\x*\x*\x/8) node[right] {$y=g(x)$};
			\draw[black] (0,0) circle (3pt);
		\end{tikzpicture}
	\end{figure}
\end{column}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{Fixed point iterations}

\begin{columns}
\begin{column}{0.5\textwidth}
To find fixed points of a given function $g(x)$, we can set up an iteration:
\begin{equation*}
	x_{n+1} = g(x_n).
\end{equation*}

This iteration stops at a fixed point.
It converges if $g(x_n)$ is closer to $x^*$ than $x_n$.

\end{column}
\begin{column}{0.5\textwidth}
	\begin{figure}
		\centering
		\begin{tikzpicture}[domain=-2:2]
			\draw[blue] plot (\x,\x) node[right] {$y=x$};
			\draw[red, domain=-2:2] plot (\x,\x*\x*\x/8) node[right] {$y=g(x)$};
			\draw[black] (0,0) circle (3pt);
			\filldraw[blue] (-1.5,-1.5) circle (1pt);
			\draw[red, ->] (-1.5,-1.5) -- (-1.5, -0.42);
			\filldraw[red] (-1.5, -0.42) circle (1pt);
			\only<2->{\draw[red, ->] (-1.5, -0.42) -- (-0.42,-0.42);
			\filldraw[blue] (-0.42,-0.42) circle (1pt);}
			\only<3->{\draw[red, ->] (-0.42,-0.42) -- (-0.42,-0.009);
			\filldraw[red] (-0.42,-0.009) circle (1pt);}
			\only<4->{\draw[red, ->] (-0.42,-0.009) -- (-0.009,-0.009);
			\filldraw[blue] (-0.009,-0.009) circle (1pt);}
		\end{tikzpicture}
	\end{figure}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{When does a fixed point iteration converge in 1D?}
\begin{columns}
\begin{column}{0.5\textwidth}
Convergence of the iteration $x_{n+1} = g(x_n)$ depends on which region $(x,g(x))$ lies.

\begin{description}
    \item[1:] Monotonic divergence
    \item[2:] Monotonic convergence
    \item[3:] Oscillatory convergence
    \item[4:] Oscillatory divergence
\end{description}
\end{column}
\begin{column}{0.5\textwidth}
    \begin{figure}
        \centering
        \begin{tikzpicture}[scale=0.6]
            \draw[black,thick] (-4,0) -- (4,0);
            \draw[black,thick] (0,-4) -- (0,4);
            \draw[black,dashed] (-4,-4) -- (4,4);
            \draw[black,dashed] (-4,4) -- (4,-4);
            
            \filldraw[black] (-1,-2) circle (3pt);
            \draw[red,->] (-1,-2) -- (-2,-2) -- (-2,-4);
            
            \filldraw[black] (-2,-1) circle (3pt);
            \draw[red,->] (-2,-1) -- (-1,-1) -- (-1,-0.5);
            
            \filldraw[black] (-2,1) circle (3pt);
            \draw[red,->] (-2,1) -- (1,1) -- (1,0.5);
            
            \filldraw[black] (-1,2) circle (3pt);
            \draw[red,->] (-1,2) -- (2,2) -- (2,4);
            
            \draw (-1,-3) node {1};
            \draw (-3,-1.5) node {2};
            \draw (-3,1.5) node {3};
            \draw (-1,3) node {4};
            \draw(1,3) node {1};
            \draw (3,1.5) node {2};
            \draw (3,-1.5) node {3};
            \draw (1,-3) node {4};
        \end{tikzpicture}
        \label{fig:FP}
    \end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Fixed point iterations in higher dimensions}

Above 1D, fixed points of a vector-valued multivariate function $\vec{g}(\vec{x})$ satisfy
\begin{equation*}
	\vec{g}(\vec{x}^*) = \vec{x}^*.
\end{equation*}

Fixed point iterations are defined as
\begin{equation*}
	\vec{x}_{n+1} = \vec{g} ( \vec{x}_n).
\end{equation*}

\end{frame}

\begin{frame}
\frametitle{When does a fixed point iteration converge in nD?}

Convergence can be shown with the Banach fixed-point theorem, which in this context requires
\begin{equation*}
	\norm{\vec{g}(\vec{x}) - \vec{g}(\vec{y})} \leq q \norm{\vec{x} - \vec{y}}, \quad q \in [0,1).
\end{equation*}

In the `regional' framework from 1D, we require
\begin{equation*}
	\norm{\vec{x}^* - \vec{g}(\vec{x}_n)} < \norm{\vec{x}^* - \vec{x}_n} .
\end{equation*}
This distinguishes between convergence and divergence, but monotonicity and oscillations are now harder to recognize.
\end{frame}

\begin{frame}
\frametitle{Numerical methods as fixed point iterations}

Almost every iterative method can be expressed as a fixed point iteration.

For example, consider Gauss-Seidel applied to:
\begin{equation*}
	\begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} \vec{u} \\ \vec{v} \end{bmatrix} = \begin{bmatrix} \vec{a} \\ \vec{b} \end{bmatrix}
\end{equation*}
\begin{equation*}
	\vec{u}_{n+1} = A^{-1} \left ( \vec{a} - B \vec{v}_n \right ), \quad \vec{v}_{n+1} = D^{-1} \left ( \vec{b} - C \vec{u}_{n+1} \right )
\end{equation*}
\begin{equation*}
	\implies \vec{v}_{n+1} = D^{-1} \left ( \vec{b} - C A^{-1} \left ( \vec{a} - B \vec{v}_n \right ) \right ) = \vec{g} ( \vec{v}_n )
\end{equation*}

\end{frame}

\begin{frame}
\frametitle{Continuous fixed point iterations}

We can represent a fixed point iteration as the numerical integration of a function:
\begin{equation*}
	\frac{\vec{x}_{n+1} - \vec{x}_n}{\Delta t} = \vec{g}(\vec{x}_n) - \vec{x}_n, \quad \Delta t = 1,
\end{equation*}
\begin{equation*}
	\implies \frac{d \vec{x}}{dt} = \vec{g}(\vec{x}) - \vec{x}.
\end{equation*}

\end{frame}

\section{Newton's method}

\begin{frame}
\frametitle{Newton's method (in 1D)}

Newton's method is used to find a root of a given function $f(x)$:
\begin{equation*}
	x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}.
\end{equation*}

This can be viewed as a fixed point iteration:
\begin{equation*}
	g_f(x) = x - \frac{f(x)}{f'(x)},
\end{equation*}
then a fixed point of $g_f(x)$ is a root of $f(x)$.

\end{frame}

\begin{frame}{When does Newton's method converge in 1D?}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            For Newton's method the regions of $g_f(x)$ depend on the slope of $f(x)$.
            
            Most of the boundaries between the regions are known problems for Newton's method:
            a slope of zero divides regions 1 and 4, and an infinite slope divides 1 and 2.
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \begin{tikzpicture}[scale=0.7]
                    \draw[black,thick] (-4,0) -- (4,0);
                    \draw[black,thick] (0,-2) -- (0,4);
                    \filldraw[black] (-2,2) circle (3pt);
                    \draw[red,->] (-4,2) -- (4,2);
                    \draw[red,->] (-2,2) -- (-2,0);
                    \draw[red,->] (-2,2) -- (0,0);
                    \draw[red,->] (-2,2) -- (2,0);
                    
                    \draw (-1.25,0.5) node {2};
                    \draw (-3,1) node {1};
                    \draw (0.25,0.5) node {3};
                    \draw (1,1.25) node {4};
                \end{tikzpicture}
                \label{fig:NR1}
            \end{figure}
        \end{column}
    \end{columns}
    % Repeat the definitions of each region
\end{frame}

\begin{frame}
\frametitle{Cycles in Newton's method}

\begin{columns}
\begin{column}{0.5\textwidth}
The line between regions 3 and 4 can cause cycles in fixed point iterations.
For Newton's method, this happens when $f(x)$ is parallel to
\begin{equation*}
	f_C(x) = C \sqrt{ \abs{x - x^*} }
\end{equation*}
for some constant $C \in \bbr$.
\end{column}
\begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \begin{tikzpicture}[scale=0.6]
                \clip (-4,-4) rectangle (4,4);
                    \draw[black,thick,->] (-4,0) -- (4,0) node[above left] {$x$};
                    \draw[black,thick,->] (0,-4) -- (0,4) node[below right] {$f(x)$};
                    \foreach \a in {1,-1,2,-2,0.5,-0.4,1.5,-1.5} {
                    \draw[domain=-4:4, smooth, samples=200, variable=\x, black, dashed] plot ({\x},{\a * sqrt(abs(\x))});
                    }
                    \draw[domain=-4:4,samples=200,variable=\x,red,thick] plot({\x},{-rad(atan(\x))-\x});
                \end{tikzpicture}
                \label{fig:NR2}
            \end{figure}
\end{column}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{Continuous Newton's method}

We can represent Newton's method as the numerical integration of an ODE:
\begin{equation*}
	\frac{x_{n+1} - x_n}{\Delta t} = -\frac{f(x_n)}{f'(x_n)}, \quad \Delta t = 1,
\end{equation*}
\begin{equation*}
	\implies \frac{\partial f}{\partial x} \frac{d x}{dt} = -f(x) \implies f(x(t)) = f(x(0)) e^{-t}.
\end{equation*}

\end{frame}

\begin{frame}
\frametitle{Newton's method in higher dimensions}

Newton's method in higher dimensions requires the Jacobian of the function, $J_f(\vec{x})$:
\begin{equation*}
	\vec{x}_{n+1} = \vec{x}_n - J_f^{-1}(\vec{x}_n) \vec{f}(\vec{x}_n).
\end{equation*}

The Kantorovich Theorem tells us this method converges as long as the initial guess is sufficiently close to the root (amongst other assumptions).

\end{frame}

\begin{frame}
\frametitle{Davidenko-Branin trick}

Davidenko (1953) and Branin (1972) suggest an update to Newton's method:
\begin{equation*}
	\frac{d \vec{x}}{dt} = \frac{ \adj J_f}{\abs{ \det J_f}} \vec{f}(\vec{x}),
\end{equation*}
using some numerical integration scheme (the update is the absolute value around $\det J_f$).

Because $\det J_f$ only changes sign when passing over a root, this version of Newton's method will always travel in the same direction between roots.
This allows the method to go over `humps' in the function that would cause Newton to diverge otherwise.

\end{frame}

\section{Acceleration}

\begin{frame}
\frametitle{Reposing a fixed point as a root}

Given a function $\vec{g}(\vec{x})$ with a fixed point $\vec{x}^*$, we can make a function with a root:
\begin{equation*}
	\vec{f}(\vec{x}) = \vec{g}(\vec{x}) - \vec{x}.
\end{equation*}

There are an infinite number of ways to construct such a function, but this is the simplest.

\end{frame}

\begin{frame}{Newton-accelerated FP in 1D}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            Apply the Newton analysis to the function $g(x)$.
            The boundaries of divergence are now when the slope of $g(x)$ is infinite, 1, or between 1 and parallel to
            \begin{equation*}
            	g_C(x) = C \sqrt{ \abs{ x - x^*}} + x
	    \end{equation*}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \begin{tikzpicture}[scale=0.6]
                \clip (-4,-4) rectangle (4,4);
                    \draw[black,thick,->] (-4,-4) -- (4,4) node[below left] {$y=x$};
                    \draw[black,thick,->] (0,-4) -- (0,4) node[below left] {$g(x)$};
                    \foreach \a in {1,-1,2,-2,0.5,-0.4,1.5,-1.5} {
                    \draw[domain=-4:4, smooth, samples=200, variable=\x, black, dashed] plot ({\x},{\a * sqrt(abs(\x)) + \x});
                    }
                    \draw[domain=-4:4, smooth, samples=200, variable=\x, red, thick] plot ({\x},{-rad(atan(\x))});
                    \end{tikzpicture}
                \label{fig:NRFP}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
\frametitle{When does Newton-accelerated FP converge in 1D?}

We can show necessary and sufficient conditions for when accelerating by Newton will guarantee convergence, based on the behaviour of the iterative method.
\\~\\
\centering
    \begin{tabular}{c|c|c}
        $g(x)$ lies in & Necessary condition & Sufficient condition \\ \hline
        1 & $g'(x) >1$ \\
        2 & $g'(x) < 1$ & $g'(x) < 1/2$ \\
        3 & $g'(x) < 1/2$ & $g'(x) < 0$ \\
        4 & $g'(x) < 0$
    \end{tabular}
\end{frame}

\begin{frame}
\frametitle{Convergent iterative methods}

It is sometimes possible to prove the guaranteed monotonic convergence of iterative methods, i.e. Schwarz methods with certain PDEs.

This means the fixed point for these methods is a contraction mapping, putting it in region 2 in the above framework.

\end{frame}

\begin{frame}
\frametitle{A basic algorithm for 1D}

Suppose $g(x)$ lies in region 2.
Start with some initial guess $x_0$.
\begin{enumerate}
\item If $g'(x_n) = 1$, then accelerating with Newton will cause a division by infinity $\to$ use the fixed point iteration
\item If $\abs{g'(x_n) - 1} < 1/2$, then using Newton with the Davidenko-Branin trick is guaranteed to be convergent $\to$ use Newton
\item Let $\tilde{x}$ be the point halfway between $x_n$ and the Newton step; if the sign of $g(\tilde{x}) - \tilde{x}$ is the same as $g(x_n) - x_n$, then the fixed point lies between $\tilde{x}$ and the Newton step $\to$ use Newton
\item If none of these are true, use the fixed point iteration
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{What about higher dimensions?}

This algorithm relies on the analysis of the boundary between regions 3 and 4.
In 1D, that was lines parallel to $C \sqrt{\abs{x-x^*}}$.

In higher dimensions, the boundary is significantly more complicated.
It may not be possible to extract necessary and sufficient conditions from this analysis.

\end{frame}

\begin{frame}
\frametitle{Augmenting Newton's method}

If an iterative method is known to be convergent, then its fixed point iteration can help anchor Newton's method.

There are only three points of interest to an augmented Newton:
\begin{itemize}
\item the current iterate, $\vec{x}_n$;
\item the fixed point step, $\vec{g}(\vec{x}_n)$;
\item the Newton step, $\vec{F}(\vec{x}_n)$ (and possibly the Newton-Davidenko-Branin step).
\end{itemize}
Thus, we need only consider the 2D plane that contains these points.
(The Newton, Newton-Davidenko-Branin, and current iterate all lie on the same line.)

\end{frame}

\begin{frame}
\frametitle{Trust region}

Since the method is convergent, $\vec{g}(\vec{x}_n)$ lies closer to $\vec{x}^*$ than $\vec{x}_n$.

% nb: figure

\end{frame}

\begin{frame}
\frametitle{Line search}

The fixed point step must have a reasonable step size, unlike Newton which may leap a great distance away.
But the Newton direction may be preferable.

% nb: figure

\end{frame}

\begin{frame}
\frametitle{Two-step}

Take the fixed point step, then step towards the Newton step, but only part of the way.

% nb: figure

\end{frame}

\end{document}